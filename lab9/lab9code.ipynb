{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e942d79-6f17-4f01-a6c1-33582e1dcc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original class distribution:\n",
      "Call_Type\n",
      "RUM    469\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Filtered to classes with >= 20 samples\n",
      "Number of samples: 469\n",
      "Number of classes: 1\n",
      "Class distribution:\n",
      "Call_Type\n",
      "RUM    469\n",
      "Name: count, dtype: int64\n",
      "======================================================================\n",
      "A1: STACKING CLASSIFIER WITH MULTIPLE META-MODELS\n",
      "======================================================================\n",
      "Error with Logistic Regression: The number of classes has to be greater than one; got 1 class\n",
      "Error with Random Forest: The number of classes has to be greater than one; got 1 class\n",
      "Error with XGBoost: The number of classes has to be greater than one; got 1 class\n",
      "Error with Decision Tree: The number of classes has to be greater than one; got 1 class\n",
      "\n",
      "Meta-Model Comparison:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "A2: PIPELINE IMPLEMENTATION\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 438\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;66;03m# Create and evaluate pipeline\u001b[39;00m\n\u001b[32m    437\u001b[39m pipeline = create_ml_pipeline(best_rf, y_train)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m pipeline_results = \u001b[43mevaluate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPipeline Performance:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    441\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_results[\u001b[33m'\u001b[39m\u001b[33mtrain_accuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 230\u001b[39m, in \u001b[36mevaluate_pipeline\u001b[39m\u001b[34m(pipeline, X_train, y_train, X_test, y_test)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03mTrain and evaluate the pipeline.\u001b[39;00m\n\u001b[32m    220\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m \u001b[33;03mresults: Dictionary containing evaluation metrics\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Fit pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[32m    233\u001b[39m y_pred_train = pipeline.predict(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/pipeline.py:663\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    658\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    659\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    660\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    661\u001b[39m             all_params=params,\n\u001b[32m    662\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/ensemble/_stacking.py:706\u001b[39m, in \u001b[36mStackingClassifier.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;28mself\u001b[39m.classes_ = \u001b[38;5;28mself\u001b[39m._label_encoder.classes_\n\u001b[32m    704\u001b[39m     y_encoded = \u001b[38;5;28mself\u001b[39m._label_encoder.transform(y)\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/ensemble/_stacking.py:211\u001b[39m, in \u001b[36m_BaseStacking.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    206\u001b[39m             \u001b[38;5;28mself\u001b[39m.estimators_.append(estimator)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimators_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_single_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdrop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28mself\u001b[39m.named_estimators_ = Bunch()\n\u001b[32m    220\u001b[39m est_fitted_idx = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/ensemble/_base.py:39\u001b[39m, in \u001b[36m_fit_single_estimator\u001b[39m\u001b[34m(estimator, X, y, fit_params, message_clsname, message)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/svm/_base.py:207\u001b[39m, in \u001b[36mBaseLibSVM.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     X, y = validate_data(\n\u001b[32m    198\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    199\u001b[39m         X,\n\u001b[32m   (...)\u001b[39m\u001b[32m    204\u001b[39m         accept_large_sparse=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    205\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m sample_weight = np.asarray(\n\u001b[32m    210\u001b[39m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype=np.float64\n\u001b[32m    211\u001b[39m )\n\u001b[32m    212\u001b[39m solver_type = LIBSVM_IMPL.index(\u001b[38;5;28mself\u001b[39m._impl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/mise/installs/python/3.13.7/lib/python3.13/site-packages/sklearn/svm/_base.py:751\u001b[39m, in \u001b[36mBaseSVC._validate_targets\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28mself\u001b[39m.class_weight_ = compute_class_weight(\u001b[38;5;28mself\u001b[39m.class_weight, classes=\u001b[38;5;28mcls\u001b[39m, y=y_)\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) < \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    752\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m class\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    753\u001b[39m         % \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    754\u001b[39m     )\n\u001b[32m    756\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = \u001b[38;5;28mcls\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(y, dtype=np.float64, order=\u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Fix for Python 3.13 multiprocessing issues\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '1'\n",
    "\n",
    "# ==================== A1: STACKING CLASSIFIER ====================\n",
    "def create_stacking_classifier(best_rf):\n",
    "    \"\"\"\n",
    "    Create a stacking classifier with multiple base models and meta-model.\n",
    "    \n",
    "    Parameters:\n",
    "    best_rf: Pre-tuned Random Forest classifier\n",
    "    \n",
    "    Returns:\n",
    "    stacking_clf: Configured StackingClassifier\n",
    "    \"\"\"\n",
    "    # Define base estimators\n",
    "    base_estimators = [\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('rf', best_rf),\n",
    "        ('svm', SVC(probability=True, random_state=42)),\n",
    "        ('nb', GaussianNB()),\n",
    "        ('mlp', MLPClassifier(random_state=42, max_iter=500)),\n",
    "        ('xgb', XGBClassifier(eval_metric='mlogloss', random_state=42)),\n",
    "        ('catboost', CatBoostClassifier(verbose=0, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Create stacking classifier with Logistic Regression as meta-model\n",
    "    # Use StratifiedKFold to ensure all classes are in each fold\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_estimators,\n",
    "        final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "        cv=cv,\n",
    "        stack_method='auto',\n",
    "        n_jobs=1  # Avoid multiprocessing issues\n",
    "    )\n",
    "    \n",
    "    return stacking_clf\n",
    "\n",
    "\n",
    "def experiment_metamodels(X_train, y_train, X_test, y_test, best_rf):\n",
    "    \"\"\"\n",
    "    Experiment with different meta-models for stacking.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train, y_train: Training data\n",
    "    X_test, y_test: Test data\n",
    "    best_rf: Pre-tuned Random Forest classifier\n",
    "    \n",
    "    Returns:\n",
    "    results_dict: Dictionary containing performance of different meta-models\n",
    "    best_stacking: Best performing stacking classifier\n",
    "    \"\"\"\n",
    "    # Define base estimators\n",
    "    base_estimators = [\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('rf', best_rf),\n",
    "        ('svm', SVC(probability=True, random_state=42)),\n",
    "        ('nb', GaussianNB()),\n",
    "        ('mlp', MLPClassifier(random_state=42, max_iter=500)),\n",
    "        ('xgb', XGBClassifier(eval_metric='mlogloss', random_state=42, verbosity=0)),\n",
    "        ('catboost', CatBoostClassifier(verbose=0, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Different meta-models to experiment\n",
    "    meta_models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(eval_metric='mlogloss', random_state=42, verbosity=0),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results_dict = {}\n",
    "    best_score = 0\n",
    "    best_stacking = None\n",
    "    \n",
    "    # Determine optimal number of splits based on smallest class\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    min_class_samples = counts.min()\n",
    "    n_splits = min(3, min_class_samples)  # Use at most 3 splits, or fewer if needed\n",
    "    \n",
    "    if n_splits < 2:\n",
    "        print(f\"Warning: Smallest class has only {min_class_samples} samples.\")\n",
    "        print(\"Using simple train-test split instead of cross-validation.\")\n",
    "        use_cv = False\n",
    "    else:\n",
    "        use_cv = True\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, meta_model in meta_models.items():\n",
    "        try:\n",
    "            if use_cv:\n",
    "                stacking_clf = StackingClassifier(\n",
    "                    estimators=base_estimators,\n",
    "                    final_estimator=meta_model,\n",
    "                    cv=cv,\n",
    "                    stack_method='auto',\n",
    "                    n_jobs=1\n",
    "                )\n",
    "            else:\n",
    "                # Without CV, train base estimators on full training set\n",
    "                stacking_clf = StackingClassifier(\n",
    "                    estimators=base_estimators,\n",
    "                    final_estimator=meta_model,\n",
    "                    cv='prefit',  # Assumes base estimators are already fitted\n",
    "                    stack_method='auto',\n",
    "                    n_jobs=1\n",
    "                )\n",
    "            \n",
    "            # Train stacking classifier\n",
    "            stacking_clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = stacking_clf.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Cross-validation score (only if CV is possible)\n",
    "            if use_cv:\n",
    "                cv_scores = cross_val_score(stacking_clf, X_train, y_train, cv=cv, n_jobs=1)\n",
    "                cv_mean = cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "            else:\n",
    "                cv_mean = accuracy\n",
    "                cv_std = 0.0\n",
    "            \n",
    "            results_dict[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'cv_mean': cv_mean,\n",
    "                'cv_std': cv_std,\n",
    "                'model': stacking_clf\n",
    "            }\n",
    "            \n",
    "            # Track best model\n",
    "            if accuracy > best_score:\n",
    "                best_score = accuracy\n",
    "                best_stacking = stacking_clf\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results_dict, best_stacking\n",
    "\n",
    "\n",
    "# ==================== A2: PIPELINE IMPLEMENTATION ====================\n",
    "def create_ml_pipeline(best_rf, y_train):\n",
    "    \"\"\"\n",
    "    Create a pipeline with preprocessing and stacking classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    best_rf: Pre-tuned Random Forest classifier\n",
    "    y_train: Training labels for CV strategy\n",
    "    \n",
    "    Returns:\n",
    "    pipeline: Complete ML pipeline\n",
    "    \"\"\"\n",
    "    # Define base estimators\n",
    "    base_estimators = [\n",
    "        ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "        ('rf', best_rf),\n",
    "        ('svm', SVC(probability=True, random_state=42)),\n",
    "        ('nb', GaussianNB()),\n",
    "        ('mlp', MLPClassifier(random_state=42, max_iter=500)),\n",
    "        ('xgb', XGBClassifier(eval_metric='mlogloss', random_state=42, verbosity=0)),\n",
    "        ('catboost', CatBoostClassifier(verbose=0, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Determine optimal CV splits\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    min_class_samples = counts.min()\n",
    "    n_splits = min(3, min_class_samples)\n",
    "    \n",
    "    if n_splits >= 2:\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        cv = 2  # Fallback to 2-fold if needed\n",
    "    \n",
    "    # Create stacking classifier\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=base_estimators,\n",
    "        final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "        cv=cv,\n",
    "        stack_method='auto',\n",
    "        n_jobs=1  # Avoid multiprocessing issues\n",
    "    )\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('stacking', stacking_clf)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate_pipeline(pipeline, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate the pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    pipeline: ML pipeline\n",
    "    X_train, y_train: Training data\n",
    "    X_test, y_test: Test data\n",
    "    \n",
    "    Returns:\n",
    "    results: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    # Fit pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Determine CV strategy\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    min_class_samples = counts.min()\n",
    "    n_splits = min(3, min_class_samples)\n",
    "    \n",
    "    if n_splits >= 2:\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        try:\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, n_jobs=1)\n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Cross-validation failed: {e}\")\n",
    "            cv_mean = test_accuracy\n",
    "            cv_std = 0.0\n",
    "    else:\n",
    "        print(\"Warning: Not enough samples per class for cross-validation\")\n",
    "        cv_mean = test_accuracy\n",
    "        cv_std = 0.0\n",
    "    \n",
    "    results = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'classification_report': classification_report(y_test, y_pred_test),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred_test)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==================== A3: LIME EXPLAINER ====================\n",
    "def explain_with_lime(pipeline, X_train, X_test, y_test, feature_names, class_names, num_samples=5):\n",
    "    \"\"\"\n",
    "    Use LIME to explain pipeline predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    pipeline: Trained ML pipeline\n",
    "    X_train: Training data for LIME\n",
    "    X_test: Test data to explain\n",
    "    y_test: True labels\n",
    "    feature_names: List of feature names\n",
    "    class_names: List of class names\n",
    "    num_samples: Number of samples to explain\n",
    "    \n",
    "    Returns:\n",
    "    explanations: List of LIME explanations\n",
    "    \"\"\"\n",
    "    # Create LIME explainer\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode='classification',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    explanations = []\n",
    "    \n",
    "    # Get random samples to explain\n",
    "    sample_indices = np.random.choice(len(X_test), size=min(num_samples, len(X_test)), replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        # Get instance\n",
    "        instance = X_test[idx]\n",
    "        \n",
    "        # Generate explanation\n",
    "        exp = explainer.explain_instance(\n",
    "            data_row=instance,\n",
    "            predict_fn=pipeline.predict_proba,\n",
    "            num_features=10\n",
    "        )\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = pipeline.predict([instance])[0]\n",
    "        true_label = y_test[idx] if hasattr(y_test, '__getitem__') else y_test.iloc[idx]\n",
    "        \n",
    "        explanation_data = {\n",
    "            'index': idx,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': prediction,\n",
    "            'explanation': exp,\n",
    "            'top_features': exp.as_list()\n",
    "        }\n",
    "        \n",
    "        explanations.append(explanation_data)\n",
    "    \n",
    "    return explanations\n",
    "\n",
    "\n",
    "def get_feature_importance_from_lime(explanations):\n",
    "    \"\"\"\n",
    "    Aggregate feature importance from multiple LIME explanations.\n",
    "    \n",
    "    Parameters:\n",
    "    explanations: List of LIME explanations\n",
    "    \n",
    "    Returns:\n",
    "    feature_importance: Dictionary of aggregated feature importance\n",
    "    \"\"\"\n",
    "    feature_importance = {}\n",
    "    \n",
    "    for exp_data in explanations:\n",
    "        for feature, weight in exp_data['top_features']:\n",
    "            if feature not in feature_importance:\n",
    "                feature_importance[feature] = []\n",
    "            feature_importance[feature].append(abs(weight))\n",
    "    \n",
    "    # Calculate average importance\n",
    "    avg_importance = {\n",
    "        feature: np.mean(weights) \n",
    "        for feature, weights in feature_importance.items()\n",
    "    }\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_importance = dict(sorted(avg_importance.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return sorted_importance\n",
    "\n",
    "\n",
    "# ==================== MAIN PROGRAM ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    df = pd.read_csv('20231225_dfall_obs_data_and_spectral_features_revision1_n469.csv')\n",
    "    \n",
    "    # Prepare data - adjust these based on your target variable\n",
    "    # Example: predicting Call_Type or Context\n",
    "    target_column = 'Call_Type'  # Change this to your actual target\n",
    "    \n",
    "    # Select features (spectral features)\n",
    "    feature_cols = [col for col in df.columns if col.startswith('V') or \n",
    "                    col.startswith('F') or col.startswith('M') or \n",
    "                    col in ['sprsMed', 'sprsMbw', 'sprsEqbw', 'sprsMc']]\n",
    "    \n",
    "    # Drop rows with missing values in target\n",
    "    df_clean = df.dropna(subset=[target_column])\n",
    "    \n",
    "    # Filter out classes with very few samples (less than 20 for safer CV)\n",
    "    class_counts = df_clean[target_column].value_counts()\n",
    "    print(f\"\\nOriginal class distribution:\\n{class_counts}\")\n",
    "    \n",
    "    valid_classes = class_counts[class_counts >= 20].index\n",
    "    df_clean = df_clean[df_clean[target_column].isin(valid_classes)]\n",
    "    \n",
    "    print(f\"\\nFiltered to classes with >= 20 samples\")\n",
    "    print(f\"Number of samples: {len(df_clean)}\")\n",
    "    print(f\"Number of classes: {df_clean[target_column].nunique()}\")\n",
    "    print(f\"Class distribution:\\n{df_clean[target_column].value_counts()}\")\n",
    "    \n",
    "    X = df_clean[feature_cols].fillna(0)\n",
    "    y = df_clean[target_column]\n",
    "    \n",
    "    # Encode target if it's string\n",
    "    if y.dtype == 'object':\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        class_names = label_encoder.classes_.tolist()\n",
    "    else:\n",
    "        class_names = sorted(y.unique().tolist())\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Split data with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train_np = X_train.values\n",
    "    X_test_np = X_test.values\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"A1: STACKING CLASSIFIER WITH MULTIPLE META-MODELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Assume best_rf is already trained from previous work\n",
    "    best_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Experiment with different meta-models\n",
    "    meta_results, best_stacking = experiment_metamodels(\n",
    "        X_train_np, y_train, X_test_np, y_test, best_rf\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMeta-Model Comparison:\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, metrics in meta_results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  CV Mean: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std']:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"A2: PIPELINE IMPLEMENTATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create and evaluate pipeline\n",
    "    pipeline = create_ml_pipeline(best_rf, y_train)\n",
    "    pipeline_results = evaluate_pipeline(pipeline, X_train_np, y_train, X_test_np, y_test)\n",
    "    \n",
    "    print(f\"\\nPipeline Performance:\")\n",
    "    print(f\"  Training Accuracy: {pipeline_results['train_accuracy']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {pipeline_results['test_accuracy']:.4f}\")\n",
    "    print(f\"  CV Mean: {pipeline_results['cv_mean']:.4f} (+/- {pipeline_results['cv_std']:.4f})\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(pipeline_results['classification_report'])\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(pipeline_results['confusion_matrix'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"A3: LIME EXPLANATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Generate LIME explanations\n",
    "    explanations = explain_with_lime(\n",
    "        pipeline, X_train_np, X_test_np, y_test, \n",
    "        feature_names, class_names, num_samples=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated {len(explanations)} LIME explanations\")\n",
    "    \n",
    "    for i, exp_data in enumerate(explanations, 1):\n",
    "        print(f\"\\n--- Sample {i} (Index: {exp_data['index']}) ---\")\n",
    "        print(f\"True Label: {exp_data['true_label']}\")\n",
    "        print(f\"Predicted Label: {exp_data['predicted_label']}\")\n",
    "        print(f\"Top Contributing Features:\")\n",
    "        for feature, weight in exp_data['top_features'][:5]:\n",
    "            print(f\"  {feature}: {weight:.4f}\")\n",
    "    \n",
    "    # Aggregate feature importance\n",
    "    feature_importance = get_feature_importance_from_lime(explanations)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGGREGATED FEATURE IMPORTANCE (Top 10)\")\n",
    "    print(\"=\"*70)\n",
    "    for feature, importance in list(feature_importance.items())[:10]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eafca7-f486-4cd8-93b1-3099b7acfb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
